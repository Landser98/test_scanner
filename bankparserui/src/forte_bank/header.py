#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ForteBank PDF parser
‚Üí Outputs header_df, tx_df, footer_df as separate DataFrames
"""

import re
import camelot
import pdfplumber
import pandas as pd
from pathlib import Path

# ------------------------------------------------------------------------------
# Utility functions
# ------------------------------------------------------------------------------
def clean_text(s):
    if not isinstance(s, str):
        return ""
    return re.sub(r"\s+", " ", s.strip())


def extract_field(text, pattern):
    m = re.search(pattern, text, flags=re.IGNORECASE | re.DOTALL)
    return m.group(1).strip() if m else ""

def cut_before_table_header(full_text: str) -> str:
    """
    –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤–µ—Ä—Ö–Ω—é—é —á–∞—Å—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã ‚Äì –¥–æ —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞
    '‚Ññ –ö“Ø–Ω—ñ/ “ö“±–∂–∞—Ç –ù”©–º—ñ—Ä—ñ/ ...'.
    –ï—Å–ª–∏ —Ç–∞–∫—É—é —Å—Ç—Ä–æ–∫—É –Ω–µ –Ω–∞—à–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –∫–∞–∫ –µ—Å—Ç—å.
    """
    if not full_text:
        return ""

    lines = full_text.splitlines()
    cutoff = None

    for i, raw_line in enumerate(lines):
        line = clean_text(raw_line)  # "‚Ññ –ö“Ø–Ω—ñ/ “ö“±–∂–∞—Ç –ù”©–º—ñ—Ä—ñ/ –ñ—ñ–±–µ—Ä—É—à—ñ ..."
        low = line.lower()

        # –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–∞–¥—ë–∂–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ —Ç–≤–æ–µ–π —à–∞–ø–∫–∏:
        # –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å ‚Ññ –∏ –≤ —Å—Ç—Ä–æ–∫–µ –µ—Å—Ç—å "–∫“Ø–Ω—ñ" –∏ "“õ“±–∂–∞—Ç"
        if low.startswith("‚Ññ".lower()) and "–∫“Ø–Ω—ñ" in low and "“õ“±–∂–∞—Ç" in low:
            cutoff = i
            break

    if cutoff is None:
        return full_text

    return "\n".join(lines[:cutoff])

# ------------------------------------------------------------------------------
# Header extraction
# ------------------------------------------------------------------------------
def parse_header(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        first_page = pdf.pages[0]
        full_text = first_page.extract_text() or ""

    # –æ—Ç—Ä–µ–∑–∞–µ–º –≤—Å—ë, —á—Ç–æ –Ω–∏–∂–µ —Å—Ç—Ä–æ–∫–∏ —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ —Ç–∞–±–ª–∏—Ü—ã
    text = cut_before_table_header(full_text)

    header = {
        "statement_date": extract_field(text, r"–ñ–∞—Å–∞–ª“ì–∞–Ω –∫“Ø–Ω—ñ[:\s]+([\d\. :/]+)"),
        "client_name": extract_field(
            text,
            r"–ö–ª–∏–µ–Ω—Ç/–ö–ª–∏–µ–Ω—Ç[:\s]+(.+?)(?:–ë–∞–Ω–∫|–ú–µ–∫–µ–Ω\s*–∂–∞–π—ã/–ê–¥—Ä–µ—Å|$)"
        ),
        "address": extract_field(
            text,
            r"–ú–µ–∫–µ–Ω\s*–∂–∞–π—ã/–ê–¥—Ä–µ—Å[:\s]+(.+?)(?:–ë–ò–ù|–ë–°–ù|–ò–ò–ö|–®–æ—Ç|–í–∞–ª—é—Ç–∞|$)"
        ),
        "BIN": extract_field(
            text,
            r"–ë–ò–ù.*?:\s*([0-9]{9,12})"
        ),
        "IIK": extract_field(
            text,
            r"–ò–ò–ö.*?:\s*([A-Z0-9]{16,34})"
        ),

        "BIK": extract_field(text, r"–ë–ò–ö[:\s]*([A-Z0-9]+)"),
        "currency": extract_field(text, r"–í–∞–ª—é—Ç–∞/–í–∞–ª—é—Ç–∞[:\s]+([A-Z]+)"),
        "opening_balance": extract_field(text, r"–í—Ö–æ–¥—è—â–∏–π –æ—Å—Ç–∞—Ç–æ–∫[:\s]*([\d,\.]+)"),
        "period_start": extract_field(
            text,
            r"([0-9]{2}\.[0-9]{2}\.[0-9]{4})\s+–±–∞—Å—Ç–∞–ø"
        ),
        "period_end": extract_field(
            text,
            r"–ø–æ\s*([0-9]{2}\.[0-9]{2}\.[0-9]{4})"
        ),
        "raw_header_text": text,          # —É–∂–µ –û–ß–ò–©–ï–ù–ù–´–ô –æ—Ç —à–∞–ø–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
    }

    return pd.DataFrame([header])

# ------------------------------------------------------------------------------
# Transaction table extraction
# ------------------------------------------------------------------------------
def parse_transactions(pdf_path):
    print("üìÑ Extracting transactions with Camelot...")

    tables = camelot.read_pdf(pdf_path, pages="all", flavor="lattice")
    if len(tables) == 0:
        print("‚ö†Ô∏è Lattice failed ‚Äî trying stream mode")
        tables = camelot.read_pdf(pdf_path, pages="all", flavor="stream", edge_tol=150)

    if len(tables) == 0:
        print("‚ùå No tables found.")
        return pd.DataFrame()

    df = pd.concat([t.df for t in tables], ignore_index=True)
    df = df.replace(r"^\s*$", pd.NA, regex=True).dropna(how="all")
    df.columns = [clean_text(c) for c in df.iloc[0]]
    df = df.iloc[1:].reset_index(drop=True)
    df = df.applymap(clean_text)
    return df


# ------------------------------------------------------------------------------
# Footer extraction
# ------------------------------------------------------------------------------
def parse_footer(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        last_page = pdf.pages[-1]
        text = last_page.extract_text() or ""

    footer = {
        "doc_count": extract_field(text, r"–ò—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤[:\s]*([0-9]+)"),
        "closing_balance": extract_field(text, r"–ò—Å—Ö–æ–¥—è—â–∏–π –æ—Å—Ç–∞—Ç–æ–∫[:\s]*([\d,\.]+)"),
        "raw_footer_text": text,
    }

    return pd.DataFrame([footer])
